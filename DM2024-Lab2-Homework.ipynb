{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Dylan Sienatra 施威任\n",
    "\n",
    "Student ID: 110006232\n",
    "\n",
    "GitHub ID: DylanSie\n",
    "\n",
    "Kaggle name: DYLAN SIENATRA\n",
    "\n",
    "Kaggle private scoreboard snapshot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Importing Necessary Librares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Preparing the Data\n",
    "In this part, we will just load the data first to check what the data looks like roughly from data_identification.csv, emotion_data.csv, and tweets_DM.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_identification = pd.read_csv('data_identification.csv')\n",
    "emotion_data = pd.read_csv('emotion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x227e25</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x293813</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x1e1a7e</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x2156a5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x2bb9d2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id identification\n",
       "0        0x28cc61           test\n",
       "1        0x29e452          train\n",
       "2        0x2b3819          train\n",
       "3        0x2db41f           test\n",
       "4        0x2a2acc          train\n",
       "...           ...            ...\n",
       "1867530  0x227e25          train\n",
       "1867531  0x293813          train\n",
       "1867532  0x1e1a7e          train\n",
       "1867533  0x2156a5          train\n",
       "1867534  0x2bb9d2          train\n",
       "\n",
       "[1867535 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x3140b1</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x368b73</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x296183</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2bd6e1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2ee1dd</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x38dba0</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x300ea2</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x360b99</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x22eecf</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x2fb282</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id       emotion\n",
       "0        0x3140b1       sadness\n",
       "1        0x368b73       disgust\n",
       "2        0x296183  anticipation\n",
       "3        0x2bd6e1           joy\n",
       "4        0x2ee1dd  anticipation\n",
       "...           ...           ...\n",
       "1455558  0x38dba0           joy\n",
       "1455559  0x300ea2           joy\n",
       "1455560  0x360b99          fear\n",
       "1455561  0x22eecf           joy\n",
       "1455562  0x2fb282  anticipation\n",
       "\n",
       "[1455563 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = []\n",
    "with open('tweets_DM.json', 'r') as file:\n",
    "    for line in file:\n",
    "        tweets_data.append(json.loads(line))\n",
    "tweets_df = pd.DataFrame([tweet['_source']['tweet'] for tweet in tweets_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                hashtags  tweet_id  \\\n",
       "0                             [Snapchat]  0x376b20   \n",
       "1          [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2                           [bibleverse]  0x28b412   \n",
       "3                                     []  0x1cd5b0   \n",
       "4                                     []  0x2de201   \n",
       "...                                  ...       ...   \n",
       "1867530  [mixedfeeling, butimTHATperson]  0x316b80   \n",
       "1867531                               []  0x29d0cb   \n",
       "1867532                               []  0x2a6a4f   \n",
       "1867533                               []  0x24faed   \n",
       "1867534                    [Sundayvibes]  0x34be8c   \n",
       "\n",
       "                                                      text  \n",
       "0        People who post \"add me on #Snapchat\" must be ...  \n",
       "1        @brianklaas As we see, Trump is dangerous to #...  \n",
       "2        Confident of your obedience, I write to you, k...  \n",
       "3                      Now ISSA is stalking Tasha 😂😂😂 <LH>  \n",
       "4        \"Trust is not the same as faith. A friend is s...  \n",
       "...                                                    ...  \n",
       "1867530  When you buy the last 2 tickets remaining for ...  \n",
       "1867531  I swear all this hard work gone pay off one da...  \n",
       "1867532  @Parcel2Go no card left when I wasn't in so I ...  \n",
       "1867533  Ah, corporate life, where you can date <LH> us...  \n",
       "1867534             Blessed to be living #Sundayvibes <LH>  \n",
       "\n",
       "[1867535 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocess and Cleaning\n",
    "Before doing anything with the data, I decided to merge the data identification, emotion data, and the tweets data into one dataframe so that we can visualize the data more clearly. Next, we will split the data into the training data (for the model training later on) and testing data (for submission prediction later on). \n",
    "\n",
    "The preprocess approach I used is using nltk tokenization from with countvectorizer and TF-IDF vectorizer, however after trying both of the feature engineering, I decided to use TF-IDF vectorizer because it yields better result because it is better for text analysis especially classification.\n",
    "\n",
    "Note: The preprocess part is in every section of the model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(data_identification, tweets_df, on='tweet_id', how='left')\n",
    "merged_data = pd.merge(merged_data, emotion_data, on='tweet_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "      <td>[]</td>\n",
       "      <td>@Habbo I've seen two separate colours of the e...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>Huge Respect🖒 @JohnnyVegasReal talking about l...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "      <td>[spateradio, app]</td>\n",
       "      <td>Yoooo we hit all our monthly goals with the ne...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "      <td>[]</td>\n",
       "      <td>@FoxNews @KellyannePolls No serious self respe...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>@KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x227e25</td>\n",
       "      <td>train</td>\n",
       "      <td>[rip]</td>\n",
       "      <td>@BBCBreaking Such an inspirational talented pe...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x293813</td>\n",
       "      <td>train</td>\n",
       "      <td>[libtards, Hillary, lost, sad, growup, Trump]</td>\n",
       "      <td>And still #libtards won't get off the guy's ba...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x1e1a7e</td>\n",
       "      <td>train</td>\n",
       "      <td>[seeds, Joy, GLTChurch]</td>\n",
       "      <td>When you sow #seeds of service or hospitality ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x2156a5</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>@lorettalrose Will you be displaying some &lt;LH&gt;...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x2bb9d2</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>Lord, I &lt;LH&gt; in you.</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id identification  \\\n",
       "0        0x28cc61           test   \n",
       "1        0x29e452          train   \n",
       "2        0x2b3819          train   \n",
       "3        0x2db41f           test   \n",
       "4        0x2a2acc          train   \n",
       "...           ...            ...   \n",
       "1867530  0x227e25          train   \n",
       "1867531  0x293813          train   \n",
       "1867532  0x1e1a7e          train   \n",
       "1867533  0x2156a5          train   \n",
       "1867534  0x2bb9d2          train   \n",
       "\n",
       "                                              hashtags  \\\n",
       "0                                                   []   \n",
       "1                                                   []   \n",
       "2                                    [spateradio, app]   \n",
       "3                                                   []   \n",
       "4                                                   []   \n",
       "...                                                ...   \n",
       "1867530                                          [rip]   \n",
       "1867531  [libtards, Hillary, lost, sad, growup, Trump]   \n",
       "1867532                        [seeds, Joy, GLTChurch]   \n",
       "1867533                                             []   \n",
       "1867534                                             []   \n",
       "\n",
       "                                                      text  emotion  \n",
       "0        @Habbo I've seen two separate colours of the e...      NaN  \n",
       "1        Huge Respect🖒 @JohnnyVegasReal talking about l...      joy  \n",
       "2        Yoooo we hit all our monthly goals with the ne...      joy  \n",
       "3        @FoxNews @KellyannePolls No serious self respe...      NaN  \n",
       "4        @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...    trust  \n",
       "...                                                    ...      ...  \n",
       "1867530  @BBCBreaking Such an inspirational talented pe...  disgust  \n",
       "1867531  And still #libtards won't get off the guy's ba...  sadness  \n",
       "1867532  When you sow #seeds of service or hospitality ...      joy  \n",
       "1867533  @lorettalrose Will you be displaying some <LH>...    trust  \n",
       "1867534                               Lord, I <LH> in you.    trust  \n",
       "\n",
       "[1867535 rows x 5 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the combined data\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test data\n",
    "train_data = merged_data[merged_data['identification'] == 'train']\n",
    "test_data = merged_data[merged_data['identification'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>Huge Respect🖒 @JohnnyVegasReal talking about l...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "      <td>[spateradio, app]</td>\n",
       "      <td>Yoooo we hit all our monthly goals with the ne...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>@KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x2a8830</td>\n",
       "      <td>train</td>\n",
       "      <td>[PUBG, GamersUnite, twitch, BeHealthy, StayPos...</td>\n",
       "      <td>Come join @ambushman27 on #PUBG while he striv...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x20b21d</td>\n",
       "      <td>train</td>\n",
       "      <td>[strength, bones, God]</td>\n",
       "      <td>@fanshixieen2014 Blessings!My #strength little...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x227e25</td>\n",
       "      <td>train</td>\n",
       "      <td>[rip]</td>\n",
       "      <td>@BBCBreaking Such an inspirational talented pe...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x293813</td>\n",
       "      <td>train</td>\n",
       "      <td>[libtards, Hillary, lost, sad, growup, Trump]</td>\n",
       "      <td>And still #libtards won't get off the guy's ba...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x1e1a7e</td>\n",
       "      <td>train</td>\n",
       "      <td>[seeds, Joy, GLTChurch]</td>\n",
       "      <td>When you sow #seeds of service or hospitality ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x2156a5</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>@lorettalrose Will you be displaying some &lt;LH&gt;...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x2bb9d2</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>Lord, I &lt;LH&gt; in you.</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id identification  \\\n",
       "1        0x29e452          train   \n",
       "2        0x2b3819          train   \n",
       "4        0x2a2acc          train   \n",
       "5        0x2a8830          train   \n",
       "6        0x20b21d          train   \n",
       "...           ...            ...   \n",
       "1867530  0x227e25          train   \n",
       "1867531  0x293813          train   \n",
       "1867532  0x1e1a7e          train   \n",
       "1867533  0x2156a5          train   \n",
       "1867534  0x2bb9d2          train   \n",
       "\n",
       "                                                  hashtags  \\\n",
       "1                                                       []   \n",
       "2                                        [spateradio, app]   \n",
       "4                                                       []   \n",
       "5        [PUBG, GamersUnite, twitch, BeHealthy, StayPos...   \n",
       "6                                   [strength, bones, God]   \n",
       "...                                                    ...   \n",
       "1867530                                              [rip]   \n",
       "1867531      [libtards, Hillary, lost, sad, growup, Trump]   \n",
       "1867532                            [seeds, Joy, GLTChurch]   \n",
       "1867533                                                 []   \n",
       "1867534                                                 []   \n",
       "\n",
       "                                                      text       emotion  \n",
       "1        Huge Respect🖒 @JohnnyVegasReal talking about l...           joy  \n",
       "2        Yoooo we hit all our monthly goals with the ne...           joy  \n",
       "4        @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...         trust  \n",
       "5        Come join @ambushman27 on #PUBG while he striv...           joy  \n",
       "6        @fanshixieen2014 Blessings!My #strength little...  anticipation  \n",
       "...                                                    ...           ...  \n",
       "1867530  @BBCBreaking Such an inspirational talented pe...       disgust  \n",
       "1867531  And still #libtards won't get off the guy's ba...       sadness  \n",
       "1867532  When you sow #seeds of service or hospitality ...           joy  \n",
       "1867533  @lorettalrose Will you be displaying some <LH>...         trust  \n",
       "1867534                               Lord, I <LH> in you.         trust  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the training data\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id          0\n",
       "identification    0\n",
       "hashtags          0\n",
       "text              0\n",
       "emotion           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see any missing values per column\n",
    "missing_values = train_data.isnull().sum()\n",
    "missing_values\n",
    "# Data looks good with no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "      <td>[]</td>\n",
       "      <td>@Habbo I've seen two separate colours of the e...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "      <td>[]</td>\n",
       "      <td>@FoxNews @KellyannePolls No serious self respe...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0x2466f6</td>\n",
       "      <td>test</td>\n",
       "      <td>[womendrivers]</td>\n",
       "      <td>Looking for a new car, and it says 1 lady owne...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0x23f9e9</td>\n",
       "      <td>test</td>\n",
       "      <td>[robbingmembers]</td>\n",
       "      <td>@cineworld “only the brave” just out and fount...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0x1fb4e1</td>\n",
       "      <td>test</td>\n",
       "      <td>[]</td>\n",
       "      <td>Felt like total dog 💩 going into open gym and ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867495</th>\n",
       "      <td>0x2c4dc2</td>\n",
       "      <td>test</td>\n",
       "      <td>[kids]</td>\n",
       "      <td>6 year old walks in astounded. Mum! Look how b...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867496</th>\n",
       "      <td>0x31be7c</td>\n",
       "      <td>test</td>\n",
       "      <td>[inspiringvolunteerawards2017]</td>\n",
       "      <td>Only one week to go until the #inspiringvolunt...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867500</th>\n",
       "      <td>0x1ca58e</td>\n",
       "      <td>test</td>\n",
       "      <td>[]</td>\n",
       "      <td>I just got caught up with the manga for \"My He...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867515</th>\n",
       "      <td>0x35c8ba</td>\n",
       "      <td>test</td>\n",
       "      <td>[]</td>\n",
       "      <td>Speak only when spoken to and make hot ass mus...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867518</th>\n",
       "      <td>0x1d941b</td>\n",
       "      <td>test</td>\n",
       "      <td>[]</td>\n",
       "      <td>Know what you want and go for it. Fuck everyon...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id identification                        hashtags  \\\n",
       "0        0x28cc61           test                              []   \n",
       "3        0x2db41f           test                              []   \n",
       "15       0x2466f6           test                  [womendrivers]   \n",
       "23       0x23f9e9           test                [robbingmembers]   \n",
       "31       0x1fb4e1           test                              []   \n",
       "...           ...            ...                             ...   \n",
       "1867495  0x2c4dc2           test                          [kids]   \n",
       "1867496  0x31be7c           test  [inspiringvolunteerawards2017]   \n",
       "1867500  0x1ca58e           test                              []   \n",
       "1867515  0x35c8ba           test                              []   \n",
       "1867518  0x1d941b           test                              []   \n",
       "\n",
       "                                                      text emotion  \n",
       "0        @Habbo I've seen two separate colours of the e...     NaN  \n",
       "3        @FoxNews @KellyannePolls No serious self respe...     NaN  \n",
       "15       Looking for a new car, and it says 1 lady owne...     NaN  \n",
       "23       @cineworld “only the brave” just out and fount...     NaN  \n",
       "31       Felt like total dog 💩 going into open gym and ...     NaN  \n",
       "...                                                    ...     ...  \n",
       "1867495  6 year old walks in astounded. Mum! Look how b...     NaN  \n",
       "1867496  Only one week to go until the #inspiringvolunt...     NaN  \n",
       "1867500  I just got caught up with the manga for \"My He...     NaN  \n",
       "1867515  Speak only when spoken to and make hot ass mus...     NaN  \n",
       "1867518  Know what you want and go for it. Fuck everyon...     NaN  \n",
       "\n",
       "[411972 rows x 5 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the testing data\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id               0\n",
       "identification         0\n",
       "hashtags               0\n",
       "text                   0\n",
       "emotion           411972\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Emotion missing is to be expected because these are the values we will try to predict but other than that we found no missing values\n",
    "missing_values_test = test_data.isnull().sum()\n",
    "missing_values_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting for Training, Validation, and Test\n",
    "In this part of the process, we will split the training data that we have into 80:20 training:validation by randomly splitting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = train_test_split(train_data, test_size=0.2, random_state=42, stratify=train_data['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1489744</th>\n",
       "      <td>0x285cdd</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>Closed Buy 1.4 Lots EURUSD 1.2022 for +11.3 pi...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825857</th>\n",
       "      <td>0x1e13d5</td>\n",
       "      <td>train</td>\n",
       "      <td>[Impressive]</td>\n",
       "      <td>@FloWrestling Ohio and PA are always 1 and 2. ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247950</th>\n",
       "      <td>0x2e186e</td>\n",
       "      <td>train</td>\n",
       "      <td>[Atlanta, Falcons, Victory, Lions, Defense, De...</td>\n",
       "      <td>#Atlanta #Falcons 3-0 hang on for the W on the...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160634</th>\n",
       "      <td>0x2eee0a</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>Love isn't about what you do for me. Love for ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619355</th>\n",
       "      <td>0x32200d</td>\n",
       "      <td>train</td>\n",
       "      <td>[sky]</td>\n",
       "      <td>Good to see Day playing like he use too.#love ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277740</th>\n",
       "      <td>0x2fd079</td>\n",
       "      <td>train</td>\n",
       "      <td>[WTF]</td>\n",
       "      <td>Hey @Colts O-line, allowing 6 sacks is not goi...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538592</th>\n",
       "      <td>0x2c8457</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>Thank you Todd Gurley for putting Rivers, Gree...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641930</th>\n",
       "      <td>0x32e118</td>\n",
       "      <td>train</td>\n",
       "      <td>[StrivingForGreatness]</td>\n",
       "      <td>5-1 in fantasy now &lt;LH&gt; #StrivingForGreatness</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536125</th>\n",
       "      <td>0x27e949</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>@kenolin1 @NBCThisisUs Got my Kleenex &lt;LH&gt;</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362092</th>\n",
       "      <td>0x21760f</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>@DonnieWahlberg I do &lt;LH&gt; to spread me some pe...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1164450 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id identification  \\\n",
       "1489744  0x285cdd          train   \n",
       "825857   0x1e13d5          train   \n",
       "1247950  0x2e186e          train   \n",
       "1160634  0x2eee0a          train   \n",
       "1619355  0x32200d          train   \n",
       "...           ...            ...   \n",
       "1277740  0x2fd079          train   \n",
       "1538592  0x2c8457          train   \n",
       "1641930  0x32e118          train   \n",
       "536125   0x27e949          train   \n",
       "362092   0x21760f          train   \n",
       "\n",
       "                                                  hashtags  \\\n",
       "1489744                                                 []   \n",
       "825857                                        [Impressive]   \n",
       "1247950  [Atlanta, Falcons, Victory, Lions, Defense, De...   \n",
       "1160634                                                 []   \n",
       "1619355                                              [sky]   \n",
       "...                                                    ...   \n",
       "1277740                                              [WTF]   \n",
       "1538592                                                 []   \n",
       "1641930                             [StrivingForGreatness]   \n",
       "536125                                                  []   \n",
       "362092                                                  []   \n",
       "\n",
       "                                                      text  emotion  \n",
       "1489744  Closed Buy 1.4 Lots EURUSD 1.2022 for +11.3 pi...      joy  \n",
       "825857   @FloWrestling Ohio and PA are always 1 and 2. ...      joy  \n",
       "1247950  #Atlanta #Falcons 3-0 hang on for the W on the...      joy  \n",
       "1160634  Love isn't about what you do for me. Love for ...      joy  \n",
       "1619355  Good to see Day playing like he use too.#love ...      joy  \n",
       "...                                                    ...      ...  \n",
       "1277740  Hey @Colts O-line, allowing 6 sacks is not goi...  disgust  \n",
       "1538592  Thank you Todd Gurley for putting Rivers, Gree...      joy  \n",
       "1641930      5-1 in fantasy now <LH> #StrivingForGreatness      joy  \n",
       "536125          @kenolin1 @NBCThisisUs Got my Kleenex <LH>    trust  \n",
       "362092   @DonnieWahlberg I do <LH> to spread me some pe...      joy  \n",
       "\n",
       "[1164450 rows x 5 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the train set\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>303298</th>\n",
       "      <td>0x252fde</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>I have the greatest wife in the world.  &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136045</th>\n",
       "      <td>0x29c585</td>\n",
       "      <td>train</td>\n",
       "      <td>[Insercure]</td>\n",
       "      <td>Cheering in my mind as the hours go by #Inserc...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71290</th>\n",
       "      <td>0x1e6dd3</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>I actually don't know what I'd do if I didn't ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745888</th>\n",
       "      <td>0x388173</td>\n",
       "      <td>train</td>\n",
       "      <td>[internet, WiFi]</td>\n",
       "      <td>Finally I have #internet &lt;LH&gt; #WiFi</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18665</th>\n",
       "      <td>0x23b95b</td>\n",
       "      <td>train</td>\n",
       "      <td>[walk, forest, sea]</td>\n",
       "      <td>What if Byron had the Iphone? &lt;LH&gt; #walk #fore...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900364</th>\n",
       "      <td>0x234e98</td>\n",
       "      <td>train</td>\n",
       "      <td>[RIPower, powermarathon, tashaAlwaysHasToclean...</td>\n",
       "      <td>Look at how the Feds messed up our house! &lt;LH&gt;...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884807</th>\n",
       "      <td>0x33a3f4</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>It's calm just calm &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490808</th>\n",
       "      <td>0x25821a</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>@data_monsters &lt;LH&gt; for the follow! Feel free ...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754413</th>\n",
       "      <td>0x302fc1</td>\n",
       "      <td>train</td>\n",
       "      <td>[sugarcon]</td>\n",
       "      <td>\"[Soon] SugarCRM will be one of the most impor...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913576</th>\n",
       "      <td>0x22ff29</td>\n",
       "      <td>train</td>\n",
       "      <td>[]</td>\n",
       "      <td>@TheRoot WOW UNBELIEVABLE...#sad</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>291113 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id identification  \\\n",
       "303298   0x252fde          train   \n",
       "1136045  0x29c585          train   \n",
       "71290    0x1e6dd3          train   \n",
       "745888   0x388173          train   \n",
       "18665    0x23b95b          train   \n",
       "...           ...            ...   \n",
       "900364   0x234e98          train   \n",
       "884807   0x33a3f4          train   \n",
       "1490808  0x25821a          train   \n",
       "1754413  0x302fc1          train   \n",
       "913576   0x22ff29          train   \n",
       "\n",
       "                                                  hashtags  \\\n",
       "303298                                                  []   \n",
       "1136045                                        [Insercure]   \n",
       "71290                                                   []   \n",
       "745888                                    [internet, WiFi]   \n",
       "18665                                  [walk, forest, sea]   \n",
       "...                                                    ...   \n",
       "900364   [RIPower, powermarathon, tashaAlwaysHasToclean...   \n",
       "884807                                                  []   \n",
       "1490808                                                 []   \n",
       "1754413                                         [sugarcon]   \n",
       "913576                                                  []   \n",
       "\n",
       "                                                      text  emotion  \n",
       "303298        I have the greatest wife in the world.  <LH>      joy  \n",
       "1136045  Cheering in my mind as the hours go by #Inserc...     fear  \n",
       "71290    I actually don't know what I'd do if I didn't ...      joy  \n",
       "745888                 Finally I have #internet <LH> #WiFi      joy  \n",
       "18665    What if Byron had the Iphone? <LH> #walk #fore...  sadness  \n",
       "...                                                    ...      ...  \n",
       "900364   Look at how the Feds messed up our house! <LH>...  disgust  \n",
       "884807                            It's calm just calm <LH>      joy  \n",
       "1490808  @data_monsters <LH> for the follow! Feel free ...    trust  \n",
       "1754413  \"[Soon] SugarCRM will be one of the most impor...    trust  \n",
       "913576                    @TheRoot WOW UNBELIEVABLE...#sad  sadness  \n",
       "\n",
       "[291113 rows x 5 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the validation set\n",
    "val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building Naive Bayes (BOW and TFIDF)\n",
    "In this part, I tried to use the same method from the Lab 2 master notebook, first by using bag of words countvectorizer that performs word frequency and use these as features to train the model. Then, using TF-IDF which converts text into term frequency inverse document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering BOW 500 features\n",
    "Here, we will tokenize using nltk.word_tokenize with 500 features as well as embed it to be feed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries for the feature engineering\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1164450, 500)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build analyzers (bag-of-words)\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "# apply analyzer to training data\n",
    "BOW_500.fit(train_set['text'])\n",
    "\n",
    "train_data_BOW_features_500 = BOW_500.transform(train_set['text'])\n",
    "\n",
    "## check dimension\n",
    "train_data_BOW_features_500.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['can', 'car', 'care', 'change', 'christ', 'christmas', 'class',\n",
       "       'closed', 'come', 'comes'], dtype=object)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe some feature names\n",
    "feature_names_500 = BOW_500.get_feature_names_out()\n",
    "feature_names_500[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (1164450, 500)\n",
      "y_train.shape:  (1164450,)\n",
      "X_test.shape:  (291113, 500)\n",
      "y_test.shape:  (291113,)\n"
     ]
    }
   ],
   "source": [
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X_train = BOW_500.transform(train_set['text'])\n",
    "y_train = train_set['emotion']\n",
    "\n",
    "X_test = BOW_500.transform(val_set['text'])\n",
    "y_test = val_set['emotion']\n",
    "\n",
    "## take a look at data dimension is a good habit  :)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model using Naive Bayes Multinomial\n",
    "Based on the results below, the accuracy is around 0.42 for both the testing and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Testing:  0.42\n",
      "Naive Bayes Accuracy Training:  0.42\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.17      0.11      0.14      7973\n",
      "anticipation       0.45      0.43      0.44     49787\n",
      "     disgust       0.29      0.32      0.30     27820\n",
      "        fear       0.18      0.13      0.15     12800\n",
      "         joy       0.49      0.63      0.55    103204\n",
      "     sadness       0.37      0.37      0.37     38687\n",
      "    surprise       0.41      0.11      0.18      9746\n",
      "       trust       0.35      0.21      0.26     41096\n",
      "\n",
      "    accuracy                           0.42    291113\n",
      "   macro avg       0.34      0.29      0.30    291113\n",
      "weighted avg       0.40      0.42      0.40    291113\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.17      0.12      0.14     31894\n",
      "anticipation       0.46      0.43      0.44    199148\n",
      "     disgust       0.29      0.32      0.30    111281\n",
      "        fear       0.18      0.13      0.15     51199\n",
      "         joy       0.49      0.63      0.55    412813\n",
      "     sadness       0.37      0.37      0.37    154750\n",
      "    surprise       0.45      0.12      0.19     38983\n",
      "       trust       0.35      0.21      0.26    164382\n",
      "\n",
      "    accuracy                           0.42   1164450\n",
      "   macro avg       0.34      0.29      0.30   1164450\n",
      "weighted avg       0.41      0.42      0.40   1164450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_nb = NB_model.predict(X_test)\n",
    "y_train_pred_nb = NB_model.predict(X_train)\n",
    "\n",
    "acc_test_nb = accuracy_score(y_test, y_test_pred_nb)\n",
    "acc_train_nb = accuracy_score(y_train, y_train_pred_nb)\n",
    "print(\"Naive Bayes Accuracy Testing: \", round(acc_test_nb, 2))\n",
    "print(\"Naive Bayes Accuracy Training: \", round(acc_train_nb, 2))\n",
    "\n",
    "print(classification_report(y_test, y_test_pred_nb))\n",
    "print(classification_report(y_train, y_train_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering using TF-IDF Vectorizer with 1000 Features\n",
    "Here, we will tokenize using nltk.word_tokenize with 1000 features as well as embed it to be feed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1164450, 1000)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF_vectorizer = TfidfVectorizer(max_features = 1000, tokenizer = nltk.word_tokenize)\n",
    "TFIDF_vectorizer.fit(train_set['text'])\n",
    "train_data_TFIDF_features = TFIDF_vectorizer.transform(train_set['text'])\n",
    "train_data_TFIDF_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (1164450, 1000)\n",
      "y_train.shape:  (1164450,)\n",
      "X_test.shape:  (291113, 1000)\n",
      "y_test.shape:  (291113,)\n"
     ]
    }
   ],
   "source": [
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X_train_TFIDF = TFIDF_vectorizer.transform(train_set['text'])\n",
    "y_train_TFIDF = train_set['emotion']\n",
    "\n",
    "X_test_TFIDF = TFIDF_vectorizer.transform(val_set['text'])\n",
    "y_test_TFIDF = val_set['emotion']\n",
    "\n",
    "## take a look at data dimension is a good habit  :)\n",
    "print('X_train.shape: ', X_train_TFIDF.shape)\n",
    "print('y_train.shape: ', y_train_TFIDF.shape)\n",
    "print('X_test.shape: ', X_test_TFIDF.shape)\n",
    "print('y_test.shape: ', y_test_TFIDF.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build The Multinomial Naive Bayes Model\n",
    "Based on the results below the testing and training has an accuracy of 0.46 which performs better than using BOW 500 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Testing:  0.46\n",
      "Naive Bayes Accuracy Training:  0.46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.89      0.04      0.07      7973\n",
      "anticipation       0.60      0.33      0.43     49787\n",
      "     disgust       0.53      0.14      0.22     27820\n",
      "        fear       0.88      0.16      0.27     12800\n",
      "         joy       0.42      0.92      0.58    103204\n",
      "     sadness       0.50      0.30      0.37     38687\n",
      "    surprise       0.85      0.07      0.13      9746\n",
      "       trust       0.73      0.07      0.12     41096\n",
      "\n",
      "    accuracy                           0.46    291113\n",
      "   macro avg       0.68      0.25      0.27    291113\n",
      "weighted avg       0.56      0.46      0.39    291113\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.88      0.04      0.08     31894\n",
      "anticipation       0.60      0.34      0.43    199148\n",
      "     disgust       0.55      0.14      0.23    111281\n",
      "        fear       0.89      0.16      0.27     51199\n",
      "         joy       0.42      0.93      0.58    412813\n",
      "     sadness       0.51      0.30      0.38    154750\n",
      "    surprise       0.88      0.08      0.14     38983\n",
      "       trust       0.74      0.07      0.12    164382\n",
      "\n",
      "    accuracy                           0.46   1164450\n",
      "   macro avg       0.68      0.26      0.28   1164450\n",
      "weighted avg       0.57      0.46      0.39   1164450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train_TFIDF, y_train_TFIDF)\n",
    "\n",
    "y_test_pred_nb_TFIDF = NB_model.predict(X_test_TFIDF)\n",
    "y_train_pred_nb_TFIDF = NB_model.predict(X_train_TFIDF)\n",
    "\n",
    "acc_test_nb_TFIDF = accuracy_score(y_test_TFIDF, y_test_pred_nb_TFIDF)\n",
    "acc_train_nb_TFIDF = accuracy_score(y_train_TFIDF, y_train_pred_nb_TFIDF)\n",
    "print(\"Naive Bayes Accuracy Testing: \", round(acc_test_nb_TFIDF, 2))\n",
    "print(\"Naive Bayes Accuracy Training: \", round(acc_train_nb_TFIDF, 2))\n",
    "\n",
    "print(classification_report(y_test_TFIDF, y_test_pred_nb_TFIDF))\n",
    "print(classification_report(y_train_TFIDF, y_train_pred_nb_TFIDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Prediction Submission\n",
    "I tried to submit the TF-IDF Multinomial Naive Bayes model and it got around 0.31 score in Kaggle, which is not that good, so I tried another approach below.\n",
    "\n",
    "Note: This is not the best prediction that was submitted. Uncomment the code below if want to make this model submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_submission_TFIDF = TFIDF_vectorizer.transform(test_data['text'])\n",
    "# submission_predictions = NB_model.predict(X_submission_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = test_data[['tweet_id']].copy()\n",
    "# submission['emotion'] = submission_predictions\n",
    "# submission.rename(columns={'tweet_id': 'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network\n",
    "I tried a different approach using DNN with Keras as the deep learning framework. In this part, I followed the template from the Lab 2 Master Notebook. I used TFIDF Vectorizer for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering using TF-IDF Vectorizer with 10.000 features, tokenized wth nltk.word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1164450, 10000)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF_vectorizer = TfidfVectorizer(max_features = 10000, tokenizer = nltk.word_tokenize)\n",
    "TFIDF_vectorizer.fit(train_set['text'])\n",
    "train_data_TFIDF_features = TFIDF_vectorizer.transform(train_set['text']) \n",
    "train_data_TFIDF_features.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Data (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (1164450, 10000)\n",
      "y_train.shape:  (1164450,)\n",
      "X_test.shape:  (291113, 10000)\n",
      "y_test.shape:  (291113,)\n"
     ]
    }
   ],
   "source": [
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X_train = TFIDF_vectorizer.transform(train_set['text']) \n",
    "y_train = train_set['emotion'] \n",
    "\n",
    "X_test = TFIDF_vectorizer.transform(val_set['text'])\n",
    "y_test = val_set['emotion']\n",
    "\n",
    "## take a look at data dimension is a good habit  :)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with categorical label (y)\n",
    "We have to process the categorical label by ourselves because we cannot directly use tran_set['emotion'] into the model. We will use one-hot encoding to encode all the possible emotions, which are anger, anticipation, disgust, fear, sadness, surprise, trust, and joy (8 in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:4]:\n",
      " 1489744    joy\n",
      "825857     joy\n",
      "1247950    joy\n",
      "1160634    joy\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1164450,)\n",
      "y_test.shape:  (291113,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:4]:\n",
      " [[0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "\n",
      "y_train.shape:  (1164450, 8)\n",
      "y_test.shape:  (291113, 8)\n"
     ]
    }
   ],
   "source": [
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Model\n",
    "In this DNN model, we have an input shape of 10000 with 2 hidden layers consisting of 64 neuron each and output layer of 8 which represents all the possible output of emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  10000\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 10000)]           0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                640064    \n",
      "                                                                 \n",
      " re_lu_6 (ReLU)              (None, 64)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " re_lu_7 (ReLU)              (None, 64)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 8)                 520       \n",
      "                                                                 \n",
      " softmax_3 (Softmax)         (None, 8)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 644744 (2.46 MB)\n",
      "Trainable params: 644744 (2.46 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 10000\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 8\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model\n",
    "In this part, I set the epochs to 5 with batch size of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "36390/36390 [==============================] - 280s 8ms/step - loss: 1.2441 - accuracy: 0.5496 - val_loss: 1.1911 - val_accuracy: 0.5673\n",
      "Epoch 2/5\n",
      "36390/36390 [==============================] - 251s 7ms/step - loss: 1.1419 - accuracy: 0.5860 - val_loss: 1.1777 - val_accuracy: 0.5734\n",
      "Epoch 3/5\n",
      "36390/36390 [==============================] - 259s 7ms/step - loss: 1.0967 - accuracy: 0.6035 - val_loss: 1.1820 - val_accuracy: 0.5737\n",
      "Epoch 4/5\n",
      "36390/36390 [==============================] - 261s 7ms/step - loss: 1.0633 - accuracy: 0.6162 - val_loss: 1.1932 - val_accuracy: 0.5722\n",
      "Epoch 5/5\n",
      "36390/36390 [==============================] - 250s 7ms/step - loss: 1.0361 - accuracy: 0.6273 - val_loss: 1.2076 - val_accuracy: 0.5699\n",
      "training finish\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_test, y_test))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on Testing Data\n",
    "In this part, we will predict the emotion for our validation set and then check the accuracy, which we achieved 0.57."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2275/2275 [==============================] - 3s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00252895, 0.05722999, 0.01630291, 0.01607423, 0.7738319 ,\n",
       "        0.01578089, 0.02313867, 0.09511235],\n",
       "       [0.04135317, 0.10112825, 0.04427046, 0.00963964, 0.54006714,\n",
       "        0.14361677, 0.04491951, 0.07500505],\n",
       "       [0.03028055, 0.04287689, 0.07142125, 0.07426297, 0.20240101,\n",
       "        0.43660122, 0.06885865, 0.0732974 ],\n",
       "       [0.07398282, 0.03299089, 0.10858422, 0.01257806, 0.62184805,\n",
       "        0.09453171, 0.01581681, 0.03966746],\n",
       "       [0.01162795, 0.07686212, 0.16254216, 0.01735673, 0.16955139,\n",
       "        0.5127418 , 0.03950766, 0.00981012]], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## predict\n",
    "pred_result = model.predict(X_test, batch_size=128)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'joy', 'sadness', 'joy', 'sadness'], dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy: 0.57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), pred_result), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.549584</td>\n",
       "      <td>1.244094</td>\n",
       "      <td>0.567264</td>\n",
       "      <td>1.191116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.585984</td>\n",
       "      <td>1.141873</td>\n",
       "      <td>0.573420</td>\n",
       "      <td>1.177691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.603513</td>\n",
       "      <td>1.096664</td>\n",
       "      <td>0.573657</td>\n",
       "      <td>1.181969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.616212</td>\n",
       "      <td>1.063311</td>\n",
       "      <td>0.572180</td>\n",
       "      <td>1.193152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.627271</td>\n",
       "      <td>1.036109</td>\n",
       "      <td>0.569861</td>\n",
       "      <td>1.207552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  accuracy      loss  val_accuracy  val_loss\n",
       "0      0  0.549584  1.244094      0.567264  1.191116\n",
       "1      1  0.585984  1.141873      0.573420  1.177691\n",
       "2      2  0.603513  1.096664      0.573657  1.181969\n",
       "3      3  0.616212  1.063311      0.572180  1.193152\n",
       "4      4  0.627271  1.036109      0.569861  1.207552"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's take a look at the training log\n",
    "training_log = pd.DataFrame()\n",
    "training_log = pd.read_csv(\"training_log.csv\")\n",
    "training_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict the Submission Testing Data\n",
    "This part, we will just predict the testing data for the submission in Kaggle and let Kaggle find the score. In Kaggle, this model reach 0.453 score which is way better than NB model.\n",
    "\n",
    "Note: Uncomment the code below to make a submission with this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_submission = TFIDF_vectorizer.transform(test_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict probabilities for each class\n",
    "# submission_pred_probs = model.predict(X_submission, batch_size=128)\n",
    "\n",
    "# # Convert probabilities to emotion labels\n",
    "# submission_pred = label_decode(label_encoder, submission_pred_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare submission dataframe\n",
    "# submission_df = test_data[['tweet_id']].copy()  # Keep only tweet_id\n",
    "# submission_df['emotion'] = submission_pred     # Add predicted emotion\n",
    "# submission_df.rename(columns={'tweet_id': 'id'}, inplace=True)\n",
    "# # Save submission to CSV\n",
    "# submission_df.to_csv('submission.csv', index=False)\n",
    "# print(\"Submission file created: 'submission.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN TFIDF with all the data\n",
    "In this section, it is the same as before but the difference is I will use all the training data without splitting it into 80:20 for training and validation in order to train the model with more data and make the model more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering using TF-IDF Vectorizer with 10.000 features, tokenized wth nltk.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1455563, 10000)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Use 10k features\n",
    "TFIDF_vectorizer = TfidfVectorizer(max_features = 10000, tokenizer = nltk.word_tokenize)\n",
    "TFIDF_vectorizer.fit(train_data['text']) \n",
    "train_data_TFIDF_features = TFIDF_vectorizer.transform(train_data['text']) \n",
    "train_data_TFIDF_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Data for DNN (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (1455563, 10000)\n",
      "y_train.shape:  (1455563,)\n"
     ]
    }
   ],
   "source": [
    "# Use all training data\n",
    "X_train = TFIDF_vectorizer.transform(train_data['text'])\n",
    "y_train = train_data['emotion']\n",
    "\n",
    "# Print data dimensions\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with categorical label (y) using one-hot encoding\n",
    "We have to process the categorical label by ourselves because we cannot directly use tran_set['emotion'] into the model. We will use one-hot encoding to encode all the possible emotions, which are anger, anticipation, disgust, fear, sadness, surprise, trust, and joy (8 in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:4]:\n",
      " 1      joy\n",
      "2      joy\n",
      "4    trust\n",
      "5      joy\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1455563,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:4]:\n",
      " [[0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "\n",
      "y_train.shape:  (1455563, 8)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Model\n",
    "\n",
    "In this DNN model, we have an input shape of 10000 with 2 hidden layers consisting of 64 neuron each and output layer of 8 which represents all the possible output of emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  10000\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 10000)]           0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                640064    \n",
      "                                                                 \n",
      " re_lu_8 (ReLU)              (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " re_lu_9 (ReLU)              (None, 64)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 8)                 520       \n",
      "                                                                 \n",
      " softmax_4 (Softmax)         (None, 8)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 644744 (2.46 MB)\n",
      "Trainable params: 644744 (2.46 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 10k\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 8\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training all the data\n",
    "\n",
    "In this part, I set the epochs to 5 with batch size of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "45487/45487 [==============================] - 297s 6ms/step - loss: 1.2313 - accuracy: 0.5544\n",
      "Epoch 2/5\n",
      "45487/45487 [==============================] - 323s 7ms/step - loss: 1.1397 - accuracy: 0.5867\n",
      "Epoch 3/5\n",
      "45487/45487 [==============================] - 382s 8ms/step - loss: 1.1025 - accuracy: 0.6011\n",
      "Epoch 4/5\n",
      "45487/45487 [==============================] - 332s 7ms/step - loss: 1.0755 - accuracy: 0.6119\n",
      "Epoch 5/5\n",
      "45487/45487 [==============================] - 342s 7ms/step - loss: 1.0543 - accuracy: 0.6203\n",
      "training finish\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger])\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict and Create Submission\n",
    "In this part, I submitted to Kaggle and got the best score of 0.458."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3219/3219 [==============================] - 6s 2ms/step\n",
      "Submission file created: 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "# Preprocess test data\n",
    "X_submission = TFIDF_vectorizer.transform(test_data['text'])\n",
    "\n",
    "# Predict test data\n",
    "submission_pred_probs = model.predict(X_submission, batch_size=128)\n",
    "submission_pred = label_decode(label_encoder, submission_pred_probs)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = test_data[['tweet_id']].copy()  # Keep only tweet_id\n",
    "submission_df['emotion'] = submission_pred     # Add predicted emotion\n",
    "submission_df.rename(columns={'tweet_id': 'id'}, inplace=True)\n",
    "# Save submission file\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created: 'submission.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
